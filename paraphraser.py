import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langgraph.graph import StateGraph, END
from typing import TypedDict

# Load environment variables
load_dotenv()
gemini_api_key = os.getenv("GOOGLE_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# Initialize LLMs
primary_llm = ChatOpenAI(
    model="openchat-3.5",
    api_key=openai_api_key,
    temperature=0.7,
    max_retries=2,
    request_timeout=30
)

secondary_llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=gemini_api_key,
    temperature=0.7
)

# --- PROMPT ---
rephrase_prompt = ChatPromptTemplate.from_messages([
    ("system", """As a master linguist and precision editor, transform the input text into an impeccably rephrased version that adheres strictly to these criteria:  

1. **Fidelity to Original Meaning**:  
   - Preserve all core concepts, technical terms, and intent without distortion.  
   - Maintain identical tone (formal/academic/casual) and nuance.  

2. **Uncompromising Originality**:  
   - Zero lexical or structural plagiarismâ€”generate entirely novel phrasing.  

3. **Conciseness & Clarity**:  
   - Eliminate redundancy, tautology, and ambiguity.  
   - Optimize sentence flow for readability while retaining depth.  

4. **Structural Rigor**:  
   - Reorganize only if it enhances logical progression.  
   - Never omit or introduce extraneous information.  

**Deliver output that is both surgically precise and elegantly articulate.**"""),
    ("human", "{input_paragraph}")
])

# --- STATE DEFINITION ---
class ParaphraserState(TypedDict):
    input_paragraph: str
    rephrased_paragraph: str
    style: str
    llm_used: str
    error: str

# --- NODE FUNCTIONS ---
def primary_llm_node(state: ParaphraserState):
    try:
        modified_input = f"[Paraphrasing Style: {state['style']}]\n\n{state['input_paragraph']}"
        response = primary_llm.invoke(rephrase_prompt.format_messages(input_paragraph=modified_input))
        return {
            "rephrased_paragraph": response.content,
            "llm_used": "ChatGPT",
            "error": None
        }
    except Exception as e:
        return {
            "llm_used": "Error",
            "error": str(e)
        }

def secondary_llm_node(state: ParaphraserState):
    try:
        modified_input = f"[Paraphrasing Style: {state['style']}]\n\n{state['input_paragraph']}"
        response = secondary_llm.invoke(rephrase_prompt.format_messages(input_paragraph=modified_input))
        return {
            "rephrased_paragraph": response.content,
            "llm_used": "Gemini",
            "error": None
        }
    except Exception as e:
        return {
            "rephrased_paragraph": "Failed to paraphrase text. Please try again.",
            "llm_used": "Error",
            "error": str(e)
        }

# --- GRAPH DEFINITION ---
builder = StateGraph(ParaphraserState)

builder.add_node("PRIMARY_LLM", RunnableLambda(primary_llm_node))
builder.add_node("SECONDARY_LLM", RunnableLambda(secondary_llm_node))

builder.set_entry_point("PRIMARY_LLM")

def route_after_primary(state: ParaphraserState):
    if state.get("error"):
        return "SECONDARY_LLM"
    return END

builder.add_conditional_edges(
    "PRIMARY_LLM",
    route_after_primary,
    {
        "SECONDARY_LLM": "SECONDARY_LLM",
        END: END
    }
)

builder.add_edge("SECONDARY_LLM", END)

# Compile the graph
paraphraser_app = builder.compile()

# Example Usage:
if __name__ == "__main__":
    user_input = input("Enter text to paraphrase:\n")
    selected_style = "Formal"  # You can change this as needed

    result = paraphraser_app.invoke({
        "input_paragraph": user_input,
        "style": selected_style,
        "rephrased_paragraph": "",
        "llm_used": "",
        "error": None
    })

    print("\n--- Paraphrased Text ---\n")
    print(result["rephrased_paragraph"])
    print(f"\n[Generated by: {result['llm_used']}]")
